{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 13: Generative AI Essentials"
      ],
      "metadata": {
        "id": "31bTjPhM-Xid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import requests\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "cPPzv6c0z3So"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Text"
      ],
      "metadata": {
        "id": "Dz2DhDJn1oZx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmmWz9n0yLV7",
        "outputId": "d6cdf49c-0d2b-4e7a-cf07-e44ad89cdc53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** START OF THE PROJECT GUTENBERG EBOOK 11 ***\n",
            "\n",
            "[Illustration]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Alice’s Adventures in Wonderland\n",
            "\n",
            "by Lewis Carroll\n",
            "\n",
            "THE MILLENNIUM FULCRUM EDITION 3.0\n",
            "\n",
            "Contents\n",
            "\n",
            " CHAPTER I.     Down the Rabbit-Hole\n",
            " CHAPTER II.    The Pool of Tears\n",
            " CHAPTER III.   A Caucus-Race and a Long Tale\n",
            " CHAPTER IV.    The Rabbit Sends in a Little Bill\n",
            " CHAPTER V.     Advice from a Caterpillar\n",
            " CHAPTER VI.    Pig and Pepper\n",
            " CHAPTER VII.   A Mad Tea-Party\n",
            " CHAPTER VIII.  The Queen’s Croquet-Ground\n",
            " CHAPTER IX.    The\n"
          ]
        }
      ],
      "source": [
        "# Download the text content directly\n",
        "url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "# Preview text\n",
        "print(text[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean the Text"
      ],
      "metadata": {
        "id": "EEABlrXO1jbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trim header/footer from Project Gutenberg\n",
        "# Trim header/footer from Project Gutenberg\n",
        "start_idx = text.find(\"CHAPTER I\")\n",
        "end_idx = text.find(\"End of the Project Gutenberg\")\n",
        "text = text[start_idx:end_idx].strip()\n",
        "\n",
        "# ✅ Reduce cleaned text size here (e.g. first 5000 characters)\n",
        "text = text[:5000]\n",
        "\n",
        "# Preview the reduced cleaned text\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnMwIS00ybtM",
        "outputId": "6b524cc5-5a45-4256-f4b1-1ce40ea52623"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHAPTER I.     Down the Rabbit-Hole\n",
            " CHAPTER II.    The Pool of Tears\n",
            " CHAPTER III.   A Caucus-Race and a Long Tale\n",
            " CHAPTER IV.    The Rabbit Sends in a Little Bill\n",
            " CHAPTER V.     Advice from a Caterpillar\n",
            " CHAPTER VI.    Pig and Pepper\n",
            " CHAPTER VII.   A Mad Tea-Party\n",
            " CHAPTER VIII.  The Queen’s Croquet-Ground\n",
            " CHAPTER IX.    The Mock Turtle’s Story\n",
            " CHAPTER X.     The Lobster Quadrille\n",
            " CHAPTER XI.    Who Stole the Tarts?\n",
            " CHAPTER XII.   Alice’s Evidence\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER I.\n",
            "Down the Rabbit-Hole\n",
            "\n",
            "\n",
            "Alice was beginning to get very tired of sitting by her sister on the\n",
            "bank, and of having nothing to do: once or twice she had peeped into\n",
            "the book her sister was reading, but it had no pictures or\n",
            "conversations in it, “and what is the use of a book,” thought Alice\n",
            "“without pictures or conversations?”\n",
            "\n",
            "So she was considering in her own mind (as well as she could, for the\n",
            "hot day made her feel very sleepy and stupid), whether the pleasure of\n",
            "making a daisy-chain would be worth the trouble of gett\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the Text for Character-Level Modeling"
      ],
      "metadata": {
        "id": "r7jsFjDO11GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of unique characters and mappings\n",
        "chars = sorted(set(text))\n",
        "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
        "idx_to_char = {i: c for c, i in char_to_idx.items()}"
      ],
      "metadata": {
        "id": "94CT3fQKy45e"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the entire text as integer indices\n",
        "encoded = [char_to_idx[c] for c in text]"
      ],
      "metadata": {
        "id": "bkJh76I16sxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input sequences and next character targets\n",
        "seq_length = 40\n",
        "X, y = [], []\n",
        "\n",
        "for i in range(len(encoded) - seq_length):\n",
        "    X.append(encoded[i:i+seq_length])\n",
        "    y.append(encoded[i+seq_length])\n",
        "\n",
        "max_samples = 5000\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "X = X[:max_samples]\n",
        "y = y[:max_samples]"
      ],
      "metadata": {
        "id": "SIOD74j660Cm"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode the labels\n",
        "y = to_categorical(y, num_classes=len(chars))"
      ],
      "metadata": {
        "id": "2Rk1MZRe65co"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build and Train the LSTM Model"
      ],
      "metadata": {
        "id": "CPfEUWHx19ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Smaller model for faster training\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(chars), output_dim=16, input_length=seq_length),  # smaller embedding\n",
        "    LSTM(64),  # fewer LSTM units\n",
        "    Dense(len(chars), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOlHXar3y8gz",
        "outputId": "7eaf029f-92ab-4a25-827a-1ac708dd59a4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - loss: 3.7710\n",
            "Epoch 2/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 3.1850\n",
            "Epoch 3/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 3.1961\n",
            "Epoch 4/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - loss: 3.1382\n",
            "Epoch 5/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 2.9998\n",
            "Epoch 6/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 2.9290\n",
            "Epoch 7/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 2.8349\n",
            "Epoch 8/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 2.7333\n",
            "Epoch 9/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 2.7096\n",
            "Epoch 10/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 2.6088\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c7bc4065550>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train fewer epochs for quick iteration\n",
        "model.fit(X, y, epochs=10, batch_size=64, verbose=1)"
      ],
      "metadata": {
        "id": "ok41GDpE7Ki2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling functions with temperature, top-k and top-p"
      ],
      "metadata": {
        "id": "UO14IzBl2UMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_top_p_filtering(logits, top_k=0, top_p=1.0):\n",
        "    logits = np.asarray(logits).astype('float64')\n",
        "    # Top-k filtering\n",
        "    if top_k > 0:\n",
        "        indices_to_remove = logits < np.partition(logits, -top_k)[-top_k]\n",
        "        logits[indices_to_remove] = -np.inf\n",
        "    # Top-p filtering\n",
        "    if top_p < 1.0:\n",
        "        sorted_indices = np.argsort(logits)[::-1]\n",
        "        sorted_logits = logits[sorted_indices]\n",
        "        cumulative_probs = np.cumsum(np.exp(sorted_logits)) / np.sum(np.exp(sorted_logits))\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        if np.any(sorted_indices_to_remove):\n",
        "            first_index = np.argmax(sorted_indices_to_remove)\n",
        "            logits[sorted_indices[first_index:]] = -np.inf\n",
        "    return logits"
      ],
      "metadata": {
        "id": "rL6ijQon7gvM"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed, length=300, temperature=1.0, top_k=0, top_p=1.0):\n",
        "    result = seed\n",
        "    for _ in range(length):\n",
        "        input_seq = [char_to_idx.get(c, 0) for c in seed[-seq_length:]]\n",
        "        input_seq = np.array(input_seq).reshape(1, -1)\n",
        "        prediction = model.predict(input_seq, verbose=0)[0]\n",
        "        prediction = np.log(prediction + 1e-9) / temperature\n",
        "        prediction = top_k_top_p_filtering(prediction, top_k=top_k, top_p=top_p) # Apply filtering\n",
        "        # Softmax normalization\n",
        "        exp_logits = np.exp(prediction - np.max(prediction))\n",
        "        probs = exp_logits / np.sum(exp_logits)\n",
        "        next_index = np.random.choice(len(probs), p=probs) # Sample from filtered probabilities\n",
        "        next_char = idx_to_char[next_index]\n",
        "        result += next_char\n",
        "        seed += next_char\n",
        "    return result"
      ],
      "metadata": {
        "id": "TpReivVR7grA"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Temperature 0.5:\\n\", generate_text(\"alice was beginning to get very \", temperature=0.5))\n",
        "print(\"\\nTemperature 1.0:\\n\", generate_text(\"alice was beginning to get very \", temperature=1.0))\n",
        "print(\"\\nTemperature 1.2:\\n\", generate_text(\"alice was beginning to get very \", temperature=1.2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQmFNxTB7gbx",
        "outputId": "e1b2ff45-8a99-47f3-9866-dea4c8c9f40e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature 0.5:\n",
            " alice was beginning to get very the hoas tin hen an thed athemen oo the the bog an seo tociw se dek there he lort the at then the showalw ware ane an the the se sor she the fon son soc he shen sot it the nren is toung wud ther the the anr toine the shon ig\n",
            "atit to anr sor as anel he wer wanur gor se the the wor ane nand ithe itof \n",
            "\n",
            "Temperature 1.0:\n",
            " alice was beginning to get very thinn\n",
            "lgaronbao\n",
            "s y yibnend ASin Rsher orasg da asas sarit HIE\n",
            "o aan —aruuth a_f f a sothi_ert fo the toE EWwiit HThe\n",
            " Gfho awaltind  t wol aal, a\n",
            "e lxbik wte Anse\n",
            "sot celt I in\n",
            "PfeHTondlt yavsa PHhor th\n",
            "Ag A. Cfnl hinl an toy ce,R ppe waRrmh de CuOl! morrEy tha wo tAlul f\n",
            "  rerul“od  Eowe ffernd bi\n",
            "\n",
            "Temperature 1.2:\n",
            " alice was beginning to get very sheka_n ac Lse d besOaser, mok sheTtufdin Ton“mPang rho thecerersh-oyl’wapiP!Terr\n",
            " f) adaVIe o liDd Rnever dhisp’t bxaret ba j nac. ho!g, ar\n",
            "anullr\n",
            "P mthe au?uk Mnit the\n",
            "th fkaws? bgits wlouAk okinseVf ilorMur tinlirP”I venpauomh shur fos\n",
            "to\n",
            "gounl hoga,g Won meak,g cey n. cuged sic, o: is nste hea m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive Widgets for prompt engineering and sampling parameters"
      ],
      "metadata": {
        "id": "DrV31OJh8LRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text input for seed\n",
        "seed_input = widgets.Text(\n",
        "    value=\"alice was beginning to get very \",\n",
        "    description='Seed:',\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")"
      ],
      "metadata": {
        "id": "pk9nwDhY8Kl_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sliders for temperature, top_k, top_p\n",
        "temperature_slider = widgets.FloatSlider(\n",
        "    value=1.0, min=0.1, max=2.0, step=0.1,\n",
        "    description='Temperature:'\n",
        ")\n",
        "\n",
        "top_k_slider = widgets.IntSlider(\n",
        "    value=10, min=0, max=50, step=1,\n",
        "    description='Top-k:'\n",
        ")\n",
        "\n",
        "top_p_slider = widgets.FloatSlider(\n",
        "    value=1.0, min=0.1, max=1.0, step=0.05,\n",
        "    description='Top-p:'\n",
        ")"
      ],
      "metadata": {
        "id": "IX3IJHKa8ddn"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Button to trigger text generation\n",
        "generate_button = widgets.Button(description=\"Generate Text\")"
      ],
      "metadata": {
        "id": "tIuzIHJF8hKs"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output area\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_generate_clicked(b):\n",
        "    output.clear_output()\n",
        "    with output:\n",
        "        generated = generate_text(\n",
        "            seed_input.value,\n",
        "            length=300,\n",
        "            temperature=temperature_slider.value,\n",
        "            top_k=top_k_slider.value,\n",
        "            top_p=top_p_slider.value\n",
        "        )\n",
        "        print(\"\\nGenerated Text:\\n\")\n",
        "        print(generated)\n",
        "\n",
        "generate_button.on_click(on_generate_clicked)"
      ],
      "metadata": {
        "id": "Kgaxzixd8msv"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt engineering"
      ],
      "metadata": {
        "id": "AHuIjfpc84Ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Example prompt engineering calls (wrapped in print for output)\n",
        "\n",
        "print(\"\\n-- Prompt Engineering Examples --\")\n",
        "\n",
        "print(\"\\nStart with action/emotion:\")\n",
        "print(generate_text(\"she ran without looking back, heart pounding. \", length=300))\n",
        "\n",
        "print(\"\\nAsk a question:\")\n",
        "print(generate_text(\"what is the meaning of the rabbit's message? \", length=300))\n",
        "\n",
        "print(\"\\nInject characters/objects:\")\n",
        "print(generate_text(\"the cat smiled as it vanished slowly, leaving \", length=300))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpIgimYJ8sW9",
        "outputId": "edb2b4ef-5021-4e09-b311-e3d8dfa989c3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Prompt Engineering Examples --\n",
            "\n",
            "Start with action/emotion:\n",
            "she ran without looking back, heart pounding. o meineeft ool boulb ’ain, w sherlet doc mti ishe s ce mey wonr\n",
            "tawot wow, wl eo Toon! tashith “e os h.h\n",
            "ther og, iwton wopsDt Sun lortoos nokeikey fad wayo Iorle thenoune HThosthen\n",
            "ce sitinr thire iRok\n",
            "otno wdis mowititighu tna aa the I”y nunw-thut uwelenl agn sogerry Ifud af waPghipl coem odl-inur\n",
            "\n",
            "Ask a question:\n",
            "what is the meaning of the rabbit's message? vadd eo?vet, “.g nde ante ors whhwts tleAtran thaby bar. wol\n",
            "sh sarggamor. In ifperltiut. e o,ky Gat” nouep kon Ronank laP wa cs ugow loun woMvergud thancewerpinf inl (am“sherd\n",
            "sodhd rumed efpud t)wopt at\n",
            "here yon i_t Wit co Wed il Aunau dod wwan Pa Polone ane gon,\n",
            "\n",
            "R\n",
            "ron ouat Vinul cou’g. sot Hherg\n",
            "\n",
            "Inject characters/objects:\n",
            "the cat smiled as it vanished slowly, leaving lod no to thekiawnde we theers i\n",
            "om f\n",
            "eaiAr annnwe\n",
            "lt aiw me no waf wogt wog THhawwunde lhec\n",
            "lbeRy . eg she so thsorn,ht polesados\n",
            "tIr odrsafd sal sr lorel. it to doe_le eIn pites way a nbnw mi“t thhat wenss thebuVtXpinnl wsannne inpk whendit\n",
            "mitoo Qant. ith borar’r  uok eon petiE\n",
            "st\n",
            "de the, dord “o\n"
          ]
        }
      ]
    }
  ]
}